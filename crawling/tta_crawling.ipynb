{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f038c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import html as LH\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from typing import Dict\n",
    "import re, os, json\n",
    "import math\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f879c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://terms.tta.or.kr/dictionary/searchFirstList.do\"\n",
    "XPATH_UL = \"/html/body/div[2]/div[2]/div/form/div/div[1]/ul\"\n",
    "DETAIL_URL = \"https://terms.tta.or.kr/dictionary/dictionarySearchFirstListAction.do\"\n",
    "\n",
    "CATEGORY_CODE = {\n",
    "    \"용어사전\": 51,\n",
    "    \"시사상식\": 107,\n",
    "    \"TTA표준\": 50,\n",
    "    \"기타참고\": 202,\n",
    "}\n",
    "\n",
    "print(\"카테고리 코드:\", CATEGORY_CODE)\n",
    "\n",
    "# 한글 초성 (ㄱ~ㅎ)\n",
    "# KOREAN_INITIALS = ['ㄱ','ㄴ','ㄷ','ㄹ','ㅁ','ㅂ','ㅅ','ㅇ','ㅈ','ㅊ','ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "KOREAN_INITIALS = ['ㅋ','ㅌ','ㅍ','ㅎ']\n",
    "\n",
    "ALPHABETS = [chr(code) for code in range(ord(\"A\"), ord(\"Z\") + 1)]\n",
    "\n",
    "DIGITS = [str(i) for i in range(10)]\n",
    "\n",
    "FIRST_LETTERS = KOREAN_INITIALS # + ALPHABETS + DIGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _with_timeout(request_fn, timeout_tuple):\n",
    "    def wrapped(method, url, **kwargs):\n",
    "        if \"timeout\" not in kwargs:\n",
    "            kwargs[\"timeout\"] = timeout_tuple\n",
    "        return request_fn(method, url, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "def build_session(timeout_connect=5, timeout_read=10, total_retries=3):\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=total_retries,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.request = _with_timeout(session.request, (timeout_connect, timeout_read))\n",
    "    return session\n",
    "\n",
    "session = build_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_payload(\n",
    "    category: str,\n",
    "    initial_consonant: str = \"ㄱ\",\n",
    "    page: int = 1,\n",
    "    list_count: int = 10,\n",
    "    search_content: str = \"conts01\",\n",
    "    orderby: str = \"kor_subject\",\n",
    "    orderby_option: str = \"TRUE\",\n",
    ") -> Dict[str, str]:\n",
    "    if category not in CATEGORY_CODE:\n",
    "        raise ValueError(f\"알 수 없는 카테고리: {category} (가능: {list(CATEGORY_CODE.keys())})\")\n",
    "    return {\n",
    "        \"searchContent\": search_content,\n",
    "        \"searchRange\": \"all\",\n",
    "        \"listCount\": str(list_count),\n",
    "        \"listPage\": str(page),\n",
    "        \"orderby\": orderby,\n",
    "        \"reFlag\": \"N\",\n",
    "        \"orderbyOption\": orderby_option,\n",
    "        \"conts01WhereSet\": \"\",\n",
    "        \"firstWordVal\": initial_consonant,\n",
    "        \"firstWord\": \"Y\",\n",
    "        \"word_seq\": \"\",\n",
    "        \"div_big_cd_in\": str(CATEGORY_CODE[category]),\n",
    "        \"div_big_cd\": \"\",\n",
    "        \"searchTerm\": \"\",\n",
    "        \"searchCate\": \"bigram\",\n",
    "    }\n",
    "\n",
    "def fetch_first_list(\n",
    "    category: str,\n",
    "    initial_consonant: str = \"ㄱ\",\n",
    "    page: int = 1,\n",
    "    list_count: int = 10,\n",
    "    search_content: str = \"conts01\",\n",
    "    orderby: str = \"kor_subject\",\n",
    "    orderby_option: str = \"TRUE\",\n",
    ") -> str:\n",
    "    payload = build_payload(category, initial_consonant, page, list_count, search_content, orderby, orderby_option)\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; TTA-Scraper/1.0)\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://terms.tta.or.kr\",\n",
    "        \"Referer\": \"https://terms.tta.or.kr/dictionary/searchFirstList.do\",\n",
    "    }\n",
    "    resp = session.post(BASE_URL, data=payload, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def fetch_detail(word_seq: str) -> dict:\n",
    "    payload = {\"word_seq\": word_seq}\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; TTA-Scraper/1.0)\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://terms.tta.or.kr\",\n",
    "        \"Referer\": \"https://terms.tta.or.kr/dictionary/searchFirstList.do\",\n",
    "    }\n",
    "\n",
    "    resp = requests.post(DETAIL_URL, data=payload, headers=headers, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    js = resp.json()\n",
    "    data = js.get(\"data\", {})\n",
    "    kor_subject = data.get(\"kor_subject\", \"\").strip()\n",
    "\n",
    "    contents_html = data.get(\"contents\", \"\").strip()\n",
    "    soup = BeautifulSoup(contents_html, \"lxml\")\n",
    "    contents = re.sub(r\"\\s+\", \" \", soup.get_text(\" \", strip=True))\n",
    "\n",
    "    return {\n",
    "        \"word_seq\": word_seq,\n",
    "        \"kor_subject\": kor_subject,\n",
    "        \"contents\": contents,\n",
    "    }\n",
    "\n",
    "def extract_total_count(html: str) -> int:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    target = soup.find(\"div\", string=re.compile(r\"총\\s*\\d+\\s*건\"))\n",
    "    if not target:\n",
    "        return 0\n",
    "    match = re.search(r\"총\\s*([0-9,]+)\\s*건\", target.get_text())\n",
    "    if match:\n",
    "        return int(match.group(1).replace(\",\", \"\"))\n",
    "    return 0\n",
    "\n",
    "def extract_first_seq_values(html_text: str) -> list:\n",
    "    tree = LH.fromstring(html_text)\n",
    "    return [v.strip() for v in tree.xpath(f\"{XPATH_UL}//input[@name='first_seq']/@value\") if v.strip()]\n",
    "\n",
    "def fetch_all_pages(category: str, initial: str, list_count: int = 10, delay: float = 0.5):\n",
    "    first_page_html = fetch_first_list(category=category, initial_consonant=initial, page=1, list_count=list_count)\n",
    "    total = extract_total_count(first_page_html)\n",
    "    if total == 0:\n",
    "        print(f\"{category}/{initial}: 결과 없음\")\n",
    "        return 0\n",
    "\n",
    "    total_pages = math.ceil(total / list_count)\n",
    "    print(f\"{category}/{initial}: 총 {total}건, {total_pages}페이지 탐색 중...\")\n",
    "\n",
    "    all_values = []\n",
    "    results = []\n",
    "    for page in range(1, total_pages + 1):\n",
    "        try:\n",
    "            html = fetch_first_list(category=category, initial_consonant=initial, page=page, list_count=list_count)\n",
    "            values = extract_first_seq_values(html)\n",
    "            print(f\"  - p{page}: {values}\")\n",
    "\n",
    "            for value in values:\n",
    "                try:\n",
    "                    data = fetch_detail(value)\n",
    "                    print(f\"   {data['word_seq']} | {data['kor_subject']}\")\n",
    "                    print(f\"   {data['contents']}\")\n",
    "                    results.append({data['kor_subject']: data['contents']})\n",
    "                    time.sleep(0.3)\n",
    "                except Exception as e:\n",
    "                    print(f\"{value} 요청 실패: {e}\")\n",
    "\n",
    "            all_values.extend(values)\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"p{page} 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ {category}/{initial}: 총 {len(all_values)}개 수집 완료\")\n",
    "\n",
    "    os.makedirs(\"tta_results\", exist_ok=True)\n",
    "    safe_cat = category.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").strip()\n",
    "    safe_init = initial.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").strip()\n",
    "    out_path = os.path.join(\"tta_results\", f\"{safe_cat}_{safe_init}.json\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"저장 완료 → {out_path}\\n\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"용어사전\", \"시사상식\", \"TTA표준\", \"기타참고\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "    print(f\"\\n=== {cat} ===\")\n",
    "    for initial in FIRST_LETTERS:\n",
    "        try:\n",
    "            fetch_all_pages(category=cat, initial=initial, list_count=10)\n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}-{initial}] 실패:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_final_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
